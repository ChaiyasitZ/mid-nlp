{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4709dc910a9e4082863555e16d9d2718": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FileUploadModel",
          "model_module_version": "1.5.0",
          "state": {
            "_counter": 11,
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FileUploadModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "FileUploadView",
            "accept": ".txt",
            "button_style": "",
            "data": [
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null
            ],
            "description": "Upload Files",
            "description_tooltip": null,
            "disabled": false,
            "error": "",
            "icon": "upload",
            "layout": "IPY_MODEL_5d7ffde89642420c8be39731e44ee375",
            "metadata": [
              {
                "name": "wiki_article.txt",
                "type": "text/plain",
                "size": 15917,
                "lastModified": 1753064794842
              },
              {
                "name": "wiki_article_0.txt",
                "type": "text/plain",
                "size": 2816,
                "lastModified": 1753064794811
              },
              {
                "name": "wiki_article_1.txt",
                "type": "text/plain",
                "size": 46665,
                "lastModified": 1753064794867
              },
              {
                "name": "wiki_article_2.txt",
                "type": "text/plain",
                "size": 2465,
                "lastModified": 1753064794862
              },
              {
                "name": "wiki_article_3.txt",
                "type": "text/plain",
                "size": 5419,
                "lastModified": 1753064794821
              },
              {
                "name": "wiki_article_4.txt",
                "type": "text/plain",
                "size": 7383,
                "lastModified": 1753064794837
              },
              {
                "name": "wiki_article_5.txt",
                "type": "text/plain",
                "size": 21470,
                "lastModified": 1753064794853
              },
              {
                "name": "wiki_article_6.txt",
                "type": "text/plain",
                "size": 18452,
                "lastModified": 1753064794847
              },
              {
                "name": "wiki_article_7.txt",
                "type": "text/plain",
                "size": 15962,
                "lastModified": 1753064794858
              },
              {
                "name": "wiki_article_8.txt",
                "type": "text/plain",
                "size": 33197,
                "lastModified": 1753064794827
              },
              {
                "name": "wiki_article_9.txt",
                "type": "text/plain",
                "size": 26993,
                "lastModified": 1753064794833
              }
            ],
            "multiple": true,
            "style": "IPY_MODEL_f77645b71f634849a266c032ea3ad636"
          }
        },
        "5d7ffde89642420c8be39731e44ee375": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f77645b71f634849a266c032ea3ad636": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "5ad1eab2be8f4a9db24da91bd1ea10bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Query:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_27c5424cc06d4a148e81a7050f57daac",
            "placeholder": "Enter your query here",
            "style": "IPY_MODEL_1423e01bef6447518204d115ba2b5b21",
            "value": "network"
          }
        },
        "27c5424cc06d4a148e81a7050f57daac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1423e01bef6447518204d115ba2b5b21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ===================================\n",
        "# Section 1: Installation of Libraries and Imports\n",
        "# ===================================\n",
        "\n",
        "# Install required libraries (run once in Google Colab)\n",
        "!pip install ipywidgets gensim\n",
        "\n",
        "# Download necessary data from NLTK\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')  # Added for wordnet\n",
        "\n",
        "# ===================================\n",
        "# Section 2: Main Libraries Import\n",
        "# ===================================\n",
        "\n",
        "# Libraries for natural language processing and data analysis\n",
        "import numpy as np\n",
        "from collections import Counter, defaultdict\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Gensim library for building topic modeling like LDA, TF-IDF\n",
        "from gensim import corpora, models, matutils\n",
        "from gensim.similarities import Similarity\n",
        "\n",
        "# Libraries for plotting graphs\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Libraries for creating interactive widgets in Jupyter Notebook\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# Libraries for data table handling\n",
        "import pandas as pd\n",
        "\n",
        "# Libraries for text similarity measurements\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.spatial.distance import euclidean, cityblock\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "# ===================================\n",
        "# Section 3: Preprocessing Functions\n",
        "# ===================================\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Function to prepare text for use\n",
        "    \"\"\"\n",
        "    try:\n",
        "        tokens = word_tokenize(text)  # Tokenize text into words\n",
        "        lower_tokens = [t.lower() for t in tokens]  # Convert all words to lowercase\n",
        "        alpha_only = [t for t in lower_tokens if t.isalpha()]  # Remove non-alphabetic tokens\n",
        "        no_stops = [t for t in alpha_only if t not in stopwords.words('english')]  # Remove stopwords\n",
        "        wordnet_lemmatizer = WordNetLemmatizer()  # Create lemmatizer object\n",
        "        words_lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]  # Lemmatize words\n",
        "        return words_lemmatized\n",
        "    except Exception as e:\n",
        "        print(f\"Error in text preprocessing: {e}\")\n",
        "        # Fallback: use simple split if tokenizer fails\n",
        "        words = text.lower().split()\n",
        "        alpha_only = [w for w in words if w.isalpha()]\n",
        "        return alpha_only\n",
        "\n",
        "def bow_to_vector(bow, all_words):\n",
        "    \"\"\"\n",
        "    Function to convert Bag-of-Words to a vector based on all words present\n",
        "    \"\"\"\n",
        "    return np.array([bow.get(word, 0) for word in all_words])\n",
        "\n",
        "# ===================================\n",
        "# Section 4: Document and Query Processing System\n",
        "# ===================================\n",
        "\n",
        "def process_documents(documents, query):\n",
        "    \"\"\"\n",
        "    Function to process all documents against the user's query\n",
        "    \"\"\"\n",
        "    bow_documents = [Counter(preprocess_text(doc)) for doc in documents]\n",
        "    bow_query = Counter(preprocess_text(query))\n",
        "\n",
        "    all_words = set(bow_query.keys())\n",
        "    for doc in bow_documents:\n",
        "        all_words.update(doc.keys())\n",
        "\n",
        "    all_words = sorted(all_words)\n",
        "\n",
        "    vectors = [bow_to_vector(bow, all_words) for bow in bow_documents]\n",
        "    query_vector = bow_to_vector(bow_query, all_words)\n",
        "\n",
        "    similarity_scores = cosine_similarity([query_vector], vectors)\n",
        "    ranked_indices = similarity_scores.argsort()[0][::-1]\n",
        "\n",
        "    results = [(idx, similarity_scores[0][idx]) for idx in ranked_indices]\n",
        "    return results\n",
        "\n",
        "# ===================================\n",
        "# Section 5: Widget Interface System\n",
        "# ===================================\n",
        "\n",
        "def setup_widgets():\n",
        "    \"\"\"\n",
        "    Function to set up all widgets\n",
        "    \"\"\"\n",
        "    global documents, query, upload_widget, query_input\n",
        "\n",
        "    # Variables to store data\n",
        "    documents = []\n",
        "    query = \"\"\n",
        "\n",
        "    # Function triggered when files are uploaded\n",
        "    def on_upload_change(change):\n",
        "        global documents\n",
        "        uploaded_files = change['new']\n",
        "        documents = []\n",
        "        for name, file in uploaded_files.items():\n",
        "            try:\n",
        "                documents.append(file['content'].decode('utf-8'))\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading file {name}: {e}\")\n",
        "        print(f\"Successfully uploaded {len(documents)} files!\")\n",
        "\n",
        "    # Function triggered when user presses Enter in the query input box\n",
        "    def on_query_submit(change):\n",
        "        global query\n",
        "        query = query_input.value\n",
        "        if not documents:\n",
        "            print(\"Please upload documents first!\")\n",
        "            return\n",
        "        if not query.strip():\n",
        "            print(\"Please enter a query!\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            results = process_documents(documents, query)\n",
        "            print(f\"\\nResults for query: '{query}'\")\n",
        "            print(\"-\" * 50)\n",
        "            for idx, score in results:\n",
        "                print(f\"Document {idx}: Similarity = {score:.4f}\")\n",
        "            print()\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing query: {e}\")\n",
        "\n",
        "    # Create widgets\n",
        "    upload_widget = widgets.FileUpload(\n",
        "        accept='.txt',\n",
        "        multiple=True,\n",
        "        description='Upload Files'\n",
        "    )\n",
        "\n",
        "    query_input = widgets.Text(\n",
        "        description='Query:',\n",
        "        placeholder='Enter your query here'\n",
        "    )\n",
        "\n",
        "    # Link event handlers\n",
        "    upload_widget.observe(on_upload_change, names='value')\n",
        "    query_input.on_submit(on_query_submit)\n",
        "\n",
        "    return upload_widget, query_input\n",
        "\n",
        "# ===================================\n",
        "# Section 6: Similarity Analysis Functions\n",
        "# ===================================\n",
        "\n",
        "def prepare_tfidf_data():\n",
        "    \"\"\"\n",
        "    Function to prepare TF-IDF data for analysis\n",
        "    \"\"\"\n",
        "    global tfidf_corpus, dictionary, documents, query\n",
        "\n",
        "    if not documents or not query:\n",
        "        print(\"Please upload documents and enter a query first!\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        # Preprocess all documents first\n",
        "        texts = [preprocess_text(doc) for doc in documents]\n",
        "        texts.append(preprocess_text(query))\n",
        "\n",
        "        # Create dictionary and corpus\n",
        "        dictionary = corpora.Dictionary(texts)\n",
        "        corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "        # Create TF-IDF model\n",
        "        tfidf_model = models.TfidfModel(corpus)\n",
        "        tfidf_corpus = tfidf_model[corpus]\n",
        "\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error preparing TF-IDF data: {e}\")\n",
        "        return False\n",
        "\n",
        "def cosine_similarity_analysis():\n",
        "    \"\"\"\n",
        "    Cosine similarity analysis\n",
        "    \"\"\"\n",
        "    if not prepare_tfidf_data():\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # Create similarity index\n",
        "        index = Similarity(output_prefix=None, corpus=tfidf_corpus, num_features=len(dictionary))\n",
        "        keyword_vector = tfidf_corpus[-1]\n",
        "\n",
        "        similarities = index[keyword_vector]\n",
        "        similarities = similarities[:-1]  # Remove the query itself\n",
        "\n",
        "        print(\"Cosine Similarity Results:\")\n",
        "        print(\"-\" * 40)\n",
        "        for i, similarity in enumerate(similarities):\n",
        "            print(f\"Query vs Document {i}: {similarity:.4f}\")\n",
        "\n",
        "        # Plot heatmap\n",
        "        similarity_matrix = np.array(similarities).reshape(1, -1)\n",
        "\n",
        "        plt.figure(figsize=(12, 3))\n",
        "        sns.heatmap(\n",
        "            similarity_matrix,\n",
        "            annot=True,\n",
        "            cmap=\"coolwarm\",\n",
        "            cbar=True,\n",
        "            xticklabels=[f'Doc {i}' for i in range(len(similarities))],\n",
        "            yticklabels=['Query']\n",
        "        )\n",
        "        plt.title(\"Cosine Similarity Heatmap\")\n",
        "        plt.xlabel(\"Documents\")\n",
        "        plt.ylabel(\"Query\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in cosine similarity analysis: {e}\")\n",
        "\n",
        "def euclidean_distance_analysis():\n",
        "    \"\"\"\n",
        "    Euclidean distance analysis\n",
        "    \"\"\"\n",
        "    if not prepare_tfidf_data():\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        def sparse_to_dense(sparse_vector, num_terms):\n",
        "            dense_vector = np.zeros(num_terms)\n",
        "            for term_id, weight in sparse_vector:\n",
        "                dense_vector[term_id] = weight\n",
        "            return dense_vector\n",
        "\n",
        "        num_terms = len(dictionary)\n",
        "        dense_tfidf_corpus = [sparse_to_dense(vector, num_terms) for vector in tfidf_corpus]\n",
        "\n",
        "        keyword_vector = dense_tfidf_corpus[-1]\n",
        "        document_vectors = dense_tfidf_corpus[:-1]\n",
        "\n",
        "        distances = [euclidean(keyword_vector, vector) for vector in document_vectors]\n",
        "\n",
        "        print(\"Euclidean Distance Results:\")\n",
        "        print(\"-\" * 40)\n",
        "        for i, distance in enumerate(distances):\n",
        "            print(f\"Query vs Document {i}: {distance:.4f}\")\n",
        "\n",
        "        # Plot heatmap\n",
        "        distance_matrix = np.array(distances).reshape(1, -1)\n",
        "\n",
        "        plt.figure(figsize=(12, 3))\n",
        "        sns.heatmap(\n",
        "            distance_matrix,\n",
        "            annot=True,\n",
        "            cmap=\"plasma\",\n",
        "            cbar=True,\n",
        "            xticklabels=[f'Doc {i}' for i in range(len(distances))],\n",
        "            yticklabels=['Query']\n",
        "        )\n",
        "        plt.title(\"Euclidean Distance Heatmap\")\n",
        "        plt.xlabel(\"Documents\")\n",
        "        plt.ylabel(\"Query\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in euclidean distance analysis: {e}\")\n",
        "\n",
        "def jaccard_similarity_analysis():\n",
        "    \"\"\"\n",
        "    TF-IDF Weighted Jaccard similarity analysis\n",
        "    \"\"\"\n",
        "    if not prepare_tfidf_data():\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        def tfidf_to_dict(tfidf_vector):\n",
        "            return dict(tfidf_vector)\n",
        "\n",
        "        query_vec = tfidf_to_dict(tfidf_corpus[-1])\n",
        "        doc_vecs = [tfidf_to_dict(vec) for vec in tfidf_corpus[:-1]]\n",
        "\n",
        "        similarities = []\n",
        "\n",
        "        for doc_vec in doc_vecs:\n",
        "            all_keys = set(query_vec.keys()) | set(doc_vec.keys())\n",
        "            numerator = sum(min(query_vec.get(k, 0), doc_vec.get(k, 0)) for k in all_keys)\n",
        "            denominator = sum(max(query_vec.get(k, 0), doc_vec.get(k, 0)) for k in all_keys)\n",
        "            score = numerator / denominator if denominator else 0\n",
        "            similarities.append(score)\n",
        "\n",
        "        print(\"TF-IDF Weighted Jaccard Similarity Results:\")\n",
        "        print(\"-\" * 50)\n",
        "        for i, sim in enumerate(similarities):\n",
        "            print(f\"Query vs Document {i}: {sim:.4f}\")\n",
        "\n",
        "        # Plot heatmap\n",
        "        similarity_matrix = np.array(similarities).reshape(1, -1)\n",
        "\n",
        "        plt.figure(figsize=(12, 3))\n",
        "        sns.heatmap(\n",
        "            similarity_matrix,\n",
        "            annot=True,\n",
        "            cmap=\"Blues\",\n",
        "            cbar=True,\n",
        "            xticklabels=[f'Doc {i}' for i in range(len(similarities))],\n",
        "            yticklabels=['Query']\n",
        "        )\n",
        "        plt.title(\"Jaccard Similarity Heatmap\")\n",
        "        plt.xlabel(\"Documents\")\n",
        "        plt.ylabel(\"Query\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in jaccard similarity analysis: {e}\")\n",
        "\n",
        "def manhattan_distance_analysis():\n",
        "    \"\"\"\n",
        "    Manhattan distance analysis\n",
        "    \"\"\"\n",
        "    if not prepare_tfidf_data():\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        dense_matrix = matutils.corpus2csc(tfidf_corpus).T.toarray()\n",
        "        norm_matrix = normalize(dense_matrix, norm='l1')\n",
        "\n",
        "        query_vec = norm_matrix[-1]\n",
        "        doc_vectors = norm_matrix[:-1]\n",
        "\n",
        "        distances = [cityblock(query_vec, doc_vec) for doc_vec in doc_vectors]\n",
        "\n",
        "        print(\"Manhattan Distance Results:\")\n",
        "        print(\"-\" * 40)\n",
        "        for i, distance in enumerate(distances):\n",
        "            print(f\"Query vs Document {i}: {distance:.4f}\")\n",
        "\n",
        "        # Plot heatmap\n",
        "        distance_matrix = np.array(distances).reshape(1, -1)\n",
        "\n",
        "        plt.figure(figsize=(12, 3))\n",
        "        sns.heatmap(\n",
        "            distance_matrix,\n",
        "            annot=True,\n",
        "            cmap=\"cool\",\n",
        "            cbar=True,\n",
        "            xticklabels=[f'Doc {i}' for i in range(len(distances))],\n",
        "            yticklabels=['Query']\n",
        "        )\n",
        "        plt.title(\"Manhattan Distance Heatmap\")\n",
        "        plt.xlabel(\"Documents\")\n",
        "        plt.ylabel(\"Query\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in manhattan distance analysis: {e}\")\n",
        "\n",
        "def run_all_analyses():\n",
        "    \"\"\"\n",
        "    Run all analyses\n",
        "    \"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"TEXT SIMILARITY ANALYSIS RESULTS\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    cosine_similarity_analysis()\n",
        "    print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
        "\n",
        "    euclidean_distance_analysis()\n",
        "    print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
        "\n",
        "    jaccard_similarity_analysis()\n",
        "    print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
        "\n",
        "    manhattan_distance_analysis()\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "\n",
        "# ===================================\n",
        "# Section 8: NLTK Issue Workaround (Backup)\n",
        "# ===================================\n",
        "\n",
        "# Backup function in case NLTK has issues\n",
        "def download_nltk_data():\n",
        "    \"\"\"\n",
        "    Download all required NLTK data\n",
        "    \"\"\"\n",
        "    resources = ['punkt', 'punkt_tab', 'stopwords', 'wordnet', 'omw-1.4']\n",
        "    for resource in resources:\n",
        "        try:\n",
        "            nltk.download(resource, quiet=True)\n",
        "            print(f\"✓ Downloaded {resource}\")\n",
        "        except Exception as e:\n",
        "            print(f\"✗ Failed to download {resource}: {e}\")\n",
        "\n",
        "# Run download immediately\n",
        "print(\"Downloading NLTK resources...\")\n",
        "download_nltk_data()\n",
        "print(\"NLTK setup complete!\")\n",
        "\n",
        "# ===================================\n",
        "# Section 7: System Initialization (Edited)\n",
        "# ===================================\n",
        "\n",
        "# Set global variables\n",
        "documents = []\n",
        "query = \"\"\n",
        "tfidf_corpus = None\n",
        "dictionary = None\n",
        "\n",
        "# Create and display widgets\n",
        "print()\n",
        "print(\"Text Similarity Analysis System\")\n",
        "print(\"=\" * 40)\n",
        "print(\"1. Upload your .txt files using the widget below\")\n",
        "print(\"2. Enter your query in the text box\")\n",
        "print(\"3. Use run_all_analyses() to see all similarity metrics\")\n",
        "print()\n",
        "\n",
        "upload_widget, query_input = setup_widgets()\n",
        "\n",
        "display(upload_widget)\n",
        "display(query_input)\n",
        "\n",
        "print(\"\\nAfter uploading files and entering query, run:\")\n",
        "print(\"run_all_analyses()\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "4709dc910a9e4082863555e16d9d2718",
            "5d7ffde89642420c8be39731e44ee375",
            "f77645b71f634849a266c032ea3ad636",
            "5ad1eab2be8f4a9db24da91bd1ea10bd",
            "27c5424cc06d4a148e81a7050f57daac",
            "1423e01bef6447518204d115ba2b5b21"
          ]
        },
        "id": "1FzSvzTnH6l8",
        "outputId": "4500d233-3a25-4858-e8c2-f318ca716e9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Results for query: 'data'\n",
            "--------------------------------------------------\n",
            "Document 6: Similarity = 0.3897\n",
            "Document 7: Similarity = 0.1659\n",
            "Document 10: Similarity = 0.1304\n",
            "Document 2: Similarity = 0.1057\n",
            "Document 5: Similarity = 0.0596\n",
            "Document 4: Similarity = 0.0492\n",
            "Document 9: Similarity = 0.0304\n",
            "Document 0: Similarity = 0.0238\n",
            "Document 8: Similarity = 0.0189\n",
            "Document 3: Similarity = 0.0000\n",
            "Document 1: Similarity = 0.0000\n",
            "\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (7.7.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (6.17.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (3.6.10)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (3.0.15)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.8.15)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (75.2.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (5.2.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets) (6.5.7)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (5.8.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.9.0.post0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.6)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (25.1.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (7.16.6)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.22.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.13)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.0->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (4.3.8)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.13.4)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.3)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.25.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (1.17.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.26.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.11/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.16.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.14.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.22)\n",
            "Requirement already satisfied: anyio>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.1)\n",
            "Downloading NLTK resources...\n",
            "✓ Downloaded punkt\n",
            "✓ Downloaded punkt_tab\n",
            "✓ Downloaded stopwords\n",
            "✓ Downloaded wordnet\n",
            "✓ Downloaded omw-1.4\n",
            "NLTK setup complete!\n",
            "\n",
            "Text Similarity Analysis System\n",
            "========================================\n",
            "1. Upload your .txt files using the widget below\n",
            "2. Enter your query in the text box\n",
            "3. Use run_all_analyses() to see all similarity metrics\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "FileUpload(value={}, accept='.txt', description='Upload Files', multiple=True)"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4709dc910a9e4082863555e16d9d2718"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Text(value='', description='Query:', placeholder='Enter your query here')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5ad1eab2be8f4a9db24da91bd1ea10bd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "After uploading files and entering query, run:\n",
            "run_all_analyses()\n",
            "Successfully uploaded 11 files!\n",
            "\n",
            "Results for query: 'computer'\n",
            "--------------------------------------------------\n",
            "Document 1: Similarity = 0.5762\n",
            "Document 5: Similarity = 0.5162\n",
            "Document 9: Similarity = 0.5060\n",
            "Document 3: Similarity = 0.3153\n",
            "Document 2: Similarity = 0.2774\n",
            "Document 4: Similarity = 0.2461\n",
            "Document 0: Similarity = 0.1668\n",
            "Document 6: Similarity = 0.1039\n",
            "Document 10: Similarity = 0.0797\n",
            "Document 8: Similarity = 0.0568\n",
            "Document 7: Similarity = 0.0207\n",
            "\n",
            "\n",
            "Results for query: 'network'\n",
            "--------------------------------------------------\n",
            "Document 8: Similarity = 0.2935\n",
            "Document 3: Similarity = 0.1802\n",
            "Document 2: Similarity = 0.1717\n",
            "Document 10: Similarity = 0.0507\n",
            "Document 6: Similarity = 0.0455\n",
            "Document 1: Similarity = 0.0360\n",
            "Document 5: Similarity = 0.0199\n",
            "Document 0: Similarity = 0.0119\n",
            "Document 7: Similarity = 0.0041\n",
            "Document 9: Similarity = 0.0000\n",
            "Document 4: Similarity = 0.0000\n",
            "\n"
          ]
        }
      ]
    }
  ]
}